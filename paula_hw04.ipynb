{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "DrxwQQXVfyT8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import math\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "pt-IZJRtfyUA"
      },
      "outputs": [],
      "source": [
        "# model of network\n",
        "class TwinModel(tf.keras.Model):\n",
        "\n",
        "    # hyperparameters are passed for instantiation \n",
        "    # to make the model esasily adjustable \n",
        "    def __init__(self, hidden_activation, output_activation, optimizer, accuracy, loss_function, depth):\n",
        "        super().__init__()\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "        self.metrics_list = [\n",
        "            tf.keras.metrics.Mean(name=\"mean\"),\n",
        "            accuracy\n",
        "            ]\n",
        "        \n",
        "        self.loss_function = loss_function\n",
        "\n",
        "        # define layers\n",
        "        self.layer1 = tf.keras.layers.Dense(128, activation=hidden_activation)\n",
        "        self.layer2 = tf.keras.layers.Dense(128, activation=hidden_activation)\n",
        "        self.output_layer = tf.keras.layers.Dense(depth, activation=output_activation)\n",
        "    \n",
        "\n",
        "    def call(self, images, training=False):\n",
        "        img1, img2 = images\n",
        "        \n",
        "        # forward pass with first image\n",
        "        x1 = self.layer1(img1)\n",
        "        x1 = self.layer2(x1)\n",
        "\n",
        "        # forward pass with second image\n",
        "        x2 = self.layer1(img2)\n",
        "        x2 = self.layer2(x2)     \n",
        "\n",
        "        combined_x = tf.concat([x1, x2], axis=1)\n",
        "\n",
        "        return self.output_layer(combined_x)\n",
        "           \n",
        "    def reset_metrics(self):\n",
        "        \n",
        "        for metric in self.metrics:\n",
        "            metric.reset_states()\n",
        "            \n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        \n",
        "        img1, img2, targets = data\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self((img1, img2), training=True)\n",
        "            \n",
        "            loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
        "        \n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update loss metric\n",
        "        self.metrics[0].update_state(loss)\n",
        "        \n",
        "        # for all metrics except loss, update states (accuracy etc.)\n",
        "        for metric in self.metrics[1:]:\n",
        "            metric.update_state(targets,predictions)\n",
        "\n",
        "        # Return a dictionary mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(self, data):\n",
        "\n",
        "        img1, img2, targets = data\n",
        "        predictions = self((img1, img2), training=False)\n",
        "        loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
        "\n",
        "        self.metrics[0].update_state(loss)\n",
        "        # for accuracy metrics:\n",
        "        for metric in self.metrics[1:]:\n",
        "            metric.update_state(targets, predictions)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "DGQHzTstfyUF"
      },
      "outputs": [],
      "source": [
        "# Define where to save the log\n",
        "config_name= \"config_name\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
        "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
        "\n",
        "# log writer for training metrics\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "\n",
        "# log writer for validation metrics\n",
        "val_summary_writer = tf.summary.create_file_writer(val_log_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "p93yNKKVfyUG"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "import tqdm\n",
        "\n",
        "def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer):\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch}:\")\n",
        "        \n",
        "        # Training:\n",
        "        for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
        "            metrics = model.train_step(data)\n",
        "            \n",
        "            # logging the validation metrics to the log file which is used by tensorboard\n",
        "            with train_summary_writer.as_default():\n",
        "                for metric in model.metrics:\n",
        "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "        # print the metrics\n",
        "        print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "        # reset all metrics (requires a reset_metrics method in the model)\n",
        "        model.reset_metrics()    \n",
        "        \n",
        "        # Validation:\n",
        "        for data in val_ds:\n",
        "            metrics = model.test_step(data)\n",
        "        \n",
        "            # logging the validation metrics to the log file which is used by tensorboard\n",
        "            with val_summary_writer.as_default():\n",
        "                for metric in model.metrics:\n",
        "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "                    \n",
        "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "        # reset all metrics\n",
        "        model.reset_metrics()\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. write function to create the dataset that we want\n",
        "def preprocess(data, batch_size, target_function, depth):\n",
        "  \n",
        "  # image should be float\n",
        "  data = data.map(lambda x, t: (tf.cast(x, float), t))\n",
        "  # image should be flattened\n",
        "  data = data.map(lambda x, t: (tf.reshape(x, (-1,)), t))\n",
        "  # image vector will here have values between -1 and 1\n",
        "  data = data.map(lambda x,t: ((x/128.)-1., t))\n",
        "  # we want to have two mnist images in each example\n",
        "  # this leads to a single example being ((x1,y1),(x2,y2))\n",
        "  zipped_ds = tf.data.Dataset.zip((data.shuffle(2000), data.shuffle(2000)))\n",
        "  #for subtask a: map ((x1,y1),(x2,y2)) to (x1,x2, y1+y2 >= 5*) *boolean\n",
        "  #for loss use binary cross entropy and for activation function sigmoid\n",
        "  #for subtask b: output layer is vector for values betweeen -9 and +9 -> turn into one_hot_vector\n",
        "  #for loss use categorical cross entropy and for activation function softmax\n",
        "  zipped_ds = zipped_ds.map(lambda x1, x2: (x1[0], x2[0], target_function(x1[1], x2[1])))\n",
        "  zipped_ds = zipped_ds.map(lambda x1, x2, target: (x1, x2, tf.one_hot(target, depth=depth)))\n",
        "\n",
        "  # batch the dataset\n",
        "  zipped_ds = zipped_ds.batch(batch_size)\n",
        "  # prefetch\n",
        "  zipped_ds = zipped_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  return zipped_ds"
      ],
      "metadata": {
        "id": "dUxtbkgGostV"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "b7316170",
        "outputId": "a863d162-e477-4b60-daf2-845a3dceebcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:41<00:00, 45.65it/s] \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['mean: 0.2108256220817566', 'acc: 0.9182500243186951']\n",
            "['val_mean: 0.1515125185251236', 'val_acc: 0.9474999904632568']\n",
            "\n",
            "\n",
            "Epoch 1:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1875/1875 [00:21<00:00, 86.65it/s] \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['mean: 0.1362280696630478', 'acc: 0.9485666751861572']\n",
            "['val_mean: 0.11767401546239853', 'val_acc: 0.9606000185012817']\n",
            "\n",
            "\n",
            "Epoch 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:40<00:00, 45.76it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mean: 0.11833494156599045', 'acc: 0.9566166400909424']\n",
            "['val_mean: 0.12575997412204742', 'val_acc: 0.9528999924659729']\n",
            "\n",
            "\n",
            "Epoch 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:21<00:00, 87.28it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mean: 0.10922941565513611', 'acc: 0.9611666798591614']\n",
            "['val_mean: 0.0931326299905777', 'val_acc: 0.9688000082969666']\n",
            "\n",
            "\n",
            "Epoch 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:21<00:00, 86.43it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mean: 0.10001623630523682', 'acc: 0.9651666879653931']\n",
            "['val_mean: 0.10495532304048538', 'val_acc: 0.9659000039100647']\n",
            "\n",
            "\n",
            "Epoch 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:21<00:00, 86.97it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mean: 0.09924587607383728', 'acc: 0.9645333290100098']\n",
            "['val_mean: 0.09567473828792572', 'val_acc: 0.9670000076293945']\n",
            "\n",
            "\n",
            "Epoch 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:40<00:00, 45.76it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mean: 0.092879518866539', 'acc: 0.9682833552360535']\n",
            "['val_mean: 0.10279272496700287', 'val_acc: 0.9678999781608582']\n",
            "\n",
            "\n",
            "Epoch 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:40<00:00, 45.76it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mean: 0.09169436991214752', 'acc: 0.9696166515350342']\n",
            "['val_mean: 0.09818533807992935', 'val_acc: 0.9685999751091003']\n",
            "\n",
            "\n",
            "Epoch 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:22<00:00, 85.16it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mean: 0.08797637373209', 'acc: 0.9706166386604309']\n",
            "['val_mean: 0.09678491950035095', 'val_acc: 0.9700999855995178']\n",
            "\n",
            "\n",
            "Epoch 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:40<00:00, 45.74it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mean: 0.08615797013044357', 'acc: 0.9713833332061768']\n",
            "['val_mean: 0.09616582095623016', 'val_acc: 0.9715999960899353']\n",
            "\n",
            "\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the functions for model.\n",
        "a = lambda x1, x2: int(x1 + x2 >= 5)\n",
        "b = lambda x1, x2: x1 - x2\n",
        "\n",
        "\n",
        "def subtask(option):\n",
        "    # 1. get mnist from tensorflow_datasets\n",
        "    train_ds, val_ds = tfds.load(\"mnist\", split =[\"train\",\"test\"], as_supervised=True)\n",
        "\n",
        "    if option == \"a\":\n",
        "        hidden_activation = tf.nn.sigmoid\n",
        "        output_activation = tf.nn.sigmoid\n",
        "        optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "        loss_function = tf.keras.losses.BinaryCrossentropy()\n",
        "        depth = 1\n",
        "        train_ds = preprocess(train_ds, batch_size=32, target_function=a, depth=depth) #train_ds.apply(preprocess)\n",
        "        val_ds = preprocess(val_ds, batch_size=32, target_function=a, depth=depth) #val_ds.apply(preprocess)\n",
        "        accuracy = tf.keras.metrics.BinaryAccuracy(name=\"acc\")\n",
        "\n",
        "    if option == \"b\":\n",
        "        hidden_activation = tf.nn.relu\n",
        "        output_activation = tf.nn.softmax\n",
        "        optimizer = tf.keras.optimizers.SGD(0.001)\n",
        "        loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "        depth=19\n",
        "        train_ds = preprocess(train_ds, batch_size=32, target_function=b, depth=depth) #train_ds.apply(preprocess)\n",
        "        val_ds = preprocess(val_ds, batch_size=32, target_function=b, depth=depth) #val_ds.apply(preprocess)\n",
        "        accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"acc\")\n",
        "\n",
        "\n",
        "    model = TwinModel(hidden_activation, output_activation, optimizer, accuracy, loss_function, depth)\n",
        "    \n",
        "    save_path = \"trained_model_RUN1\"\n",
        "    \n",
        "    training_loop(model=model,\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    epochs=10,\n",
        "    train_summary_writer=train_summary_writer,\n",
        "    val_summary_writer=val_summary_writer,\n",
        "    )\n",
        "\n",
        "\n",
        "#initialize model with corresponding functions\n",
        "#model_a = TwinMNISTModel(hidden_activation, output_activation, optimizer, loss_function)# 1. instantiate model\n",
        "\n",
        "# 2. choose a path to save the weights\n",
        "#save_path = \"trained_model_RUN1\"\n",
        "\n",
        "# 2. pass arguments to training loop function\n",
        "\n",
        "'''training_loop(model=model_a,\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    start_epoch=0,\n",
        "    epochs=10,\n",
        "    train_summary_writer=train_summary_writer,\n",
        "    val_summary_writer=val_summary_writer,\n",
        "    save_path=save_path)'''\n",
        "subtask(\"a\")\n",
        "\n",
        "print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RG6kv8VfyUI"
      },
      "source": [
        "# Saving and loading a subclassed model\n",
        "\n",
        "Because training deep neural networks can take multiple days, weeks or even months, we want to save checkpoints in between. This is especially useful if you use Google Colab and you save the model directly to your Google Drive folder. That way you don't lose any progress if your runtime gets closed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYu7MM-sfyUJ",
        "outputId": "f8689b6a-af1f-4fd7-d307-a0b7e2a92cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████▏    | 964/1875 [00:14<00:13, 66.14it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-821d78145a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 val_summary_writer=val_summary_writer)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-ecc53e94f20c>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{metric.name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# print the metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorboard/plugins/scalar/summary_v2.py\u001b[0m in \u001b[0;36mscalar\u001b[0;34m(name, data, step, description)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummary_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         )\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(tag, tensor, step, metadata, name)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     op = smart_cond.smart_cond(\n\u001b[0;32m--> 770\u001b[0;31m         should_record_summaries(), record, _nothing, name=\"summary_cond\")\n\u001b[0m\u001b[1;32m    771\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mshould_record_summaries\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mit\u001b[0m \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \"\"\"\n\u001b[0;32m--> 139\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_should_record_summaries_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36m_should_record_summaries_internal\u001b[0;34m(default_state)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond_distributed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mlogical_and\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   5693\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5694\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 5695\u001b[0;31m         _ctx, \"LogicalAnd\", name, x, y)\n\u001b[0m\u001b[1;32m   5696\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5697\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# save the model with a meaningful name\n",
        "model.save_weights(f\"saved_model_{config_name}\", save_format=\"tf\")\n",
        "\n",
        "# load the model:\n",
        "# instantiate a new model from our CNN class\n",
        "loaded_model = FFN(optimizer, loss_function)\n",
        "\n",
        "# build the model\n",
        "#inp= tf.keras.Input((28,28,1))\n",
        "#loaded_model(inp)\n",
        "\n",
        "# load the model weights to continue training. \n",
        "loaded_model.load_weights(f\"saved_model_{config_name}\");\n",
        "\n",
        "# continue training (but: optimizer state is lost)\n",
        "\n",
        "# run the training loop \n",
        "training_loop(model=loaded_model, \n",
        "                train_ds=train_ds, \n",
        "                val_ds=val_ds, \n",
        "                epochs=10, \n",
        "                train_summary_writer=train_summary_writer, \n",
        "                val_summary_writer=val_summary_writer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "54ff86533a6a943eb33cb0954e5964c6e356fb8134919fff31cf4713965c9c7c"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}