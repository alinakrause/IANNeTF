{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68bbec95-36c0-49ea-ab5f-a9277370d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c18ec65-1b35-4e62-8064-5789dd95fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input data\n",
    "input_data = np.random.rand(100)\n",
    "\n",
    "#create lambda function to apply on data\n",
    "f = lambda x: -x**3+x**2\n",
    "\n",
    "#create target set\n",
    "target = f(input_data)\n",
    "\n",
    "#define ReLU\n",
    "def reLu(x):\n",
    "    return x * (x > 0)\n",
    "def derived_reLu(x):\n",
    "    return 1 if x > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "911522de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_units, input_units):\n",
    "        self.n_units = n_units\n",
    "        self.input_units = input_units\n",
    "        \n",
    "        #bias vector initialized with zeros\n",
    "        self.bias_vector = np.zeros(n_units)\n",
    "        \n",
    "        #Xavier Initialization for Random Weights\n",
    "        self.weight_matrix = np.random.normal(0, np.sqrt(2/n_units+input_units),(input_units, n_units))\n",
    "        \n",
    "        #Initialize empty attributes\n",
    "        self.layer_input = None\n",
    "        self.layer_preactivation = None\n",
    "        self.layer_activation = None\n",
    "        \n",
    "    def forward_step(self, input_vector):\n",
    "        #get vector of (weight x input) + bias\n",
    "        self.layer_input = input_vector\n",
    "        \"\"\"print(np.shape(self.weight_matrix))\n",
    "        print(self.weight_matrix)\n",
    "        print(self.layer_input)\n",
    "        print(self.bias_vector)\"\"\"\n",
    "        self.layer_preactivation = self.weight_matrix.T @ self.layer_input + self.bias_vector\n",
    "        \"\"\"print(\"weight matrix\")\n",
    "        print(self.weight_matrix.T)\n",
    "        print(\"input\")\n",
    "        print(self.layer_input)\n",
    "        print(\"preactivation\")\n",
    "        print(self.layer_preactivation)\"\"\"\n",
    "        self.layer_activation = reLu(self.layer_preactivation)\n",
    "        #print(\"activation\")\n",
    "        #print(self.layer_activation)\n",
    "        return self.layer_activation\n",
    "        \n",
    "    def backward_step(self, activation_derivative, lr):\n",
    "        \n",
    "        #calculates gradient for weights and biases\n",
    "        bias_grad = derived_reLu(np.any(self.layer_preactivation)) * activation_derivative\n",
    "        #print(self.layer_input.T)\n",
    "        \"\"\"print(bias_grad)\n",
    "        print(np.array([bias_grad]))\n",
    "        print(np.shape(bias_grad))\n",
    "        print(self.layer_input)\n",
    "        print(np.shape(self.layer_input))\n",
    "        print(np.array([self.layer_input]).T)\n",
    "        print(np.shape(np.array([self.layer_input]).T))\"\"\"\n",
    "        weight_grad = np.array([self.layer_input]).T @ np.asarray([bias_grad])\n",
    "        \n",
    "        #calculates the derivative of the loss function with regards to the activation\n",
    "        print(self.weight_matrix)\n",
    "        new_derivative = np.asarray([bias_grad]) @ self.weight_matrix.T\n",
    "        \n",
    "        #update weights and bias\n",
    "        self.bias_vector = self.bias_vector - lr*bias_grad\n",
    "        self.weight_matrix = self.weight_matrix - lr*weight_grad\n",
    "        \n",
    "        return new_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "df2cbe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "  # initialized with learning rate, \n",
    "  # list of number of layers and their respective unit size\n",
    "    def __init__(self, lr, n_layers, units_list):\n",
    "        self.lr = lr\n",
    "        #initialize layers as list of layers\n",
    "        self.layers = []\n",
    "        self.target = target\n",
    "        self.last_output = None\n",
    "        units_list = units_list\n",
    "        n_input = 1\n",
    "        for n_units in units_list:\n",
    "            self.layers.append(Layer(n_units, n_input))\n",
    "            n_input = n_units\n",
    "    def forward_step(self, data_point):\n",
    "        #propagates input signal forward through the list by \n",
    "        #updating the input for the next layer\n",
    "        layer_input = data_point\n",
    "        for layer in self.layers:\n",
    "            layer_input = layer.forward_step(layer_input)    \n",
    "        self.last_output = layer_input\n",
    "        #print(\"last output\")\n",
    "        #print(self.last_output)\n",
    "        return self.last_output\n",
    "    def backpropagation(self, target):\n",
    "        #compute derivative for loss function\n",
    "        delta = self.last_output - target\n",
    "        #perform backpropagation by executing backward_step on each layer and updating the derivative\n",
    "        for layer in reversed(self.layers):\n",
    "            delta = layer.backward_step(delta, self.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "290be5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.44143637]\n",
      " [ 2.27588221]\n",
      " [ 5.14694155]\n",
      " [ 1.70872538]\n",
      " [-0.19047581]\n",
      " [ 1.9462242 ]\n",
      " [ 4.76330009]\n",
      " [-0.32646899]\n",
      " [ 0.45104944]\n",
      " [-8.80868893]]\n",
      "[[ 0.17894276  0.22189702 -0.49134614 -0.75717973 -0.18037082 -0.97266899\n",
      "  -1.6631664   1.44273391  0.54446453 -1.62924713]]\n",
      "[[ 5.44138585]\n",
      " [ 2.27581956]\n",
      " [ 5.14694155]\n",
      " [ 1.70872538]\n",
      " [-0.19047581]\n",
      " [ 1.9462242 ]\n",
      " [ 4.76330009]\n",
      " [-0.32687634]\n",
      " [ 0.45089571]\n",
      " [-8.80868893]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_121/4133374500.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#perform backward step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#record loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_output\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_121/3797648940.py\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#perform backpropagation by executing backward_step on each layer and updating the derivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_121/1183988136.py\u001b[0m in \u001b[0;36mbackward_step\u001b[0;34m(self, activation_derivative, lr)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#calculates the derivative of the loss function with regards to the activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mnew_derivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbias_grad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m#update weights and bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 10)"
     ]
    }
   ],
   "source": [
    "mlp = MLP(0.01, 2, [10,1])\n",
    "loss = []\n",
    "for i in range(1000):\n",
    "    for d, t in zip(input_data, target):\n",
    "        #perform forward step\n",
    "        mlp.forward_step(np.asarray([d]))\n",
    "        #perform backward step\n",
    "        mlp.backpropagation(t)\n",
    "        #record loss\n",
    "        loss.append((1/2)*(mlp.last_output - t)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "dd01d387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [4.]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1.,2.,3.,4.]])\n",
    "x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc4f7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
