{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 10 - Group 14\n",
        "\n",
        "The following code takes an English translation of the bible. It tokenizes the words and filters out punctuation and stop words (commonly occuring words that don't add much meaning). The tokens are used to train a skipgram model that uses negative sampling (with randomly chosen words) for training. The training progress is shown through a graph and tables with example words and their assigned 5 nearest neighbors during different states of the training."
      ],
      "metadata": {
        "id": "gEkErGVLjBvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 The Dataset\n",
        "The text file of the bible is taken from a connected drive and as an example to validate that it worked as intended, the first 200 characters are printed."
      ],
      "metadata": {
        "id": "tughda48muoG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh0EuKnkdLNX",
        "outputId": "0a40d04c-3ac8-4915-971e-1d23a673a161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Total Characters:  4332496\n",
            "First 200 Characters:\n",
            "The First Book of Moses:  Called Genesis\n",
            "\n",
            "\n",
            "1:1 In the beginning God created the heaven and the earth.\n",
            "\n",
            "1:2 And the earth was without form, and void; and darkness was upon\n",
            "the face of the deep. And the\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "#load data by mounting to drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "os.chdir(\"drive/MyDrive\")\n",
        "\n",
        "# read txt file\n",
        "f = open(\"bible.txt\", \"r\")\n",
        "bible = f.read()\n",
        "\n",
        "print(\"Total Characters: \", len(bible))\n",
        "print(\"First 200 Characters:\\n\" + bible[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 Word Embeddings\n",
        "A tokenizer is created that first learns a vocabulary from the bible list and then translates the bible list into a list of corresponding tokens. The tokenizer already filters out punctuation. Stop-words were taken out to make the data more meaningful. To reduce the data set further, subsampling was applied to filter out some of the most common words. Finally, context-target wordpairs are created. To utilize negative subsampling during training, for each positive sample that is created, the are five negative samples created. The pairs are matched with a corresponding label - 1 for true and 0 for false.\n"
      ],
      "metadata": {
        "id": "vY_fBst9mJEm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BjxJrt2vQc-",
        "outputId": "174108eb-6079-4efe-b9c2-3d5253c777a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of Tokens: 426142\n",
            "Vocabulary size: 9000\n",
            "First 10 Token integers: [1, 249, 360, 3, 105, 192, 6020, 67, 67, 5]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "vocabulary_size = 10000\n",
        "\n",
        "# Tokenize the text\n",
        "# automatically converts to lowercase and removes new lines and special characters\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocabulary_size) \n",
        "tokenizer.fit_on_texts([bible])\n",
        "tokens = tokenizer.texts_to_sequences([bible])[0]\n",
        "\n",
        "n_tokens = len(tokens)\n",
        "\n",
        "\n",
        "# store relative frequencies of tokens at respective index (e.g. token_frequencies[1] = frequency of token with integer=1)\n",
        "# for subsampling and negative sampling\n",
        "token_frequencies = [0]\n",
        "for word, token in tokenizer.word_index.items():\n",
        "    # get number of occurences for each token\n",
        "    frequency = tokenizer.word_counts.get(word)\n",
        "    token_frequencies.append(frequency)\n",
        "# divide by sum of all occurences\n",
        "token_frequencies = token_frequencies / np.sum(token_frequencies)\n",
        "\n",
        "\n",
        "print(\"Total number of Tokens:\", n_tokens)\n",
        "print(\"Vocabulary size:\", vocabulary_size)\n",
        "print(\"First 10 Token integers:\", tokens[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcGhofIdMD8k",
        "outputId": "cb589999-fc96-4b4e-edf1-78b63c07fd84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of Tokens: 169595\n",
            "First 10 Token integers after filtering: [360, 105, 192, 6020, 67, 67, 29, 1980, 277, 163]\n"
          ]
        }
      ],
      "source": [
        "# removing stop words to reduce dataset and make it more expressive\n",
        "# (commonly occuring words that don't carry much meaning)\n",
        "\n",
        "# translate list of stop words into tokens\n",
        "stop_words = '''\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"'''\n",
        "stop = ''.join(letter for letter in str(stop_words) if letter not in '\",')\n",
        "stop_tokens = tokenizer.texts_to_sequences([stop_words])[0]\n",
        "\n",
        "# filter bible tokens for stop words\n",
        "tokens = [token for token in tokens if token not in stop_tokens]\n",
        "n_tokens = len(tokens)\n",
        "\n",
        "print(\"Total number of Tokens:\", n_tokens)\n",
        "print(\"First 10 Token integers after filtering:\", tokens[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quCD7EprJ2Ta",
        "outputId": "f6f65d9e-8e2e-49d3-ddf2-233fce6cf1e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of Tokens: 162761\n",
            "First 10 Token integers after filtering: [360, 105, 192, 6020, 67, 67, 29, 1980, 277, 163]\n"
          ]
        }
      ],
      "source": [
        "# compute probability with which a token is kept in the dataset\n",
        "def token_probability(token, token_frequencies, scale=0.001):\n",
        "    \"\"\"Calculates the probability that a token is removed from the dataset based on its frequency in the text.\n",
        "\n",
        "    Args:\n",
        "        token(int): number that is assigned to represent the word\n",
        "        token_frequencies(list): list with the relative frequencies of all tokens at respective index\n",
        "        scale(float): constant, that controls how likely it is that a word is kept\n",
        "\n",
        "    Returns:\n",
        "        Probability that input token is kept in vocabulary\n",
        "\n",
        "    \"\"\"\n",
        "    # calculate relative frequency of token occuring in the text\n",
        "    token_frequency = token_frequencies[token]\n",
        "    # probability formula\n",
        "    probability = (np.sqrt(token_frequency / scale) + 1) * (scale / token_frequency)\n",
        "\n",
        "    return probability\n",
        "\n",
        "\n",
        "# subsampling: remove frequent tokens by certain probability\n",
        "s = 0.001\n",
        "tokens = [token for token in tokens if np.random.uniform(0, 1) < token_probability(token, token_frequencies, scale=s)]\n",
        "n_tokens = len(tokens)\n",
        "\n",
        "print(\"Total number of Tokens:\", n_tokens)\n",
        "print(\"First 10 Token integers after filtering:\", tokens[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lrcUM1EnQkS",
        "outputId": "85cb08ec-df7f-4baa-ccb2-0ebb643e7ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of Pairs: 2604152\n",
            "Example input-target pair: [62, 7458] with Label: 0\n"
          ]
        }
      ],
      "source": [
        "# Generate target-context pairs\n",
        "# performs negative sampling \n",
        "# for each positive pair sample 3 negative pairs\n",
        "pairs, labels = tf.keras.preprocessing.sequence.skipgrams(sequence=tokens, \n",
        "                                                          vocabulary_size=vocabulary_size, \n",
        "                                                          window_size=2,\n",
        "                                                          negative_samples=3 # ratio of negative compared to positive samples\n",
        "                                                          )\n",
        "\n",
        "print(\"Total number of Pairs:\", len(pairs))\n",
        "print(\"Example input-target pair:\", pairs[0], \"with Label:\", labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 The Model\n",
        "A SkipGram NN-Model is implemented with an embedding layer, a flatten layer and a dense output layer. The model predicts whether the presented pair is true or false."
      ],
      "metadata": {
        "id": "xveVTjefoqJZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBOwzRdxGheF"
      },
      "outputs": [],
      "source": [
        "# SkipGram NN model\n",
        "class SkipGram(tf.keras.Model):\n",
        "    \"\"\"Subclass of tf.keras.Model that is trained to determine if a context, target token pair is positive or negative.\n",
        "\n",
        "    Args:\n",
        "        tf.keras.Model\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_units=64):\n",
        "        \"\"\"Initializes an embedding, a flatten and a dense layer for the model\n",
        "\n",
        "        Args:\n",
        "            embedding_units(int): size of the embedding\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_units = embedding_units\n",
        "        regularizer = tf.keras.regularizers.L2() \n",
        "\n",
        "        # two main layers: embedding and dense output layer (+ in between flatten)\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocabulary_size,\n",
        "                                                   output_dim=embedding_units,\n",
        "                                                   input_length=2,\n",
        "                                                   input_shape=(2,),\n",
        "                                                   embeddings_regularizer=regularizer\n",
        "                                                   )\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.out = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        \"\"\"Passes input x through all the layers of the model.\n",
        "\n",
        "        Args:\n",
        "            self = this instance of the class\n",
        "            x (tuple of ints): context, target token pair\n",
        "\n",
        "        Returns:\n",
        "            prediction of the model \n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 Training\n",
        "A tracker for the nearest neighbors is implemented. The accuracy and loss for training and testing data are visualized and tables are printed which show the training process for 8 tracking words. Since most training seems to happen in the first epoch, there are tables printed after each 5000 batches."
      ],
      "metadata": {
        "id": "rnOB4WffqCbP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78yLmZkJFpcc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# class for function called at end of each epoch\n",
        "# tracking the k-nearest words of tracking words during training\n",
        "class TrackingNeighbors(tf.keras.callbacks.Callback):\n",
        "    \"\"\"The class provides functions to track the k-nearest neighbors of a token. Is a subclass of tf.keras.callbacks.Callback.\n",
        "\n",
        "    Args:\n",
        "         tf.keras.callbacks.Callback\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tracking_words, tokenizer, k=5):\n",
        "        \"\"\"Creates a dictionary for each word that is to be tracked.\n",
        "\n",
        "        Args:\n",
        "            self: this instance of the class\n",
        "            model(SkipGram): SkipGram Model that is being trained\n",
        "            tracking_words(list of strings): words to keep track of during training\n",
        "            tokenizer(tf.keras.preprocessing.text.Tokenizer): object that creates tokens out of strings\n",
        "            k(int): number of neighbors to return for each tracked word\n",
        "\n",
        "        Returns:\n",
        "            prediction of the model \n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        self.model = model\n",
        "        self.k = k\n",
        "        self.tracking_words = tracking_words\n",
        "\n",
        "        # dictionaries where neighbors of all tracking words get appended\n",
        "        # one for every epoch\n",
        "        self.neighbors_dict_epochs = {}\n",
        "        for word in tracking_words:\n",
        "            self.neighbors_dict_epochs[word] = [] # keys = tracking words\n",
        "        \n",
        "        # and one for intermediate samples during the first epoch\n",
        "        self.neighbors_dict_batches = {}\n",
        "        for word in tracking_words:\n",
        "            self.neighbors_dict_batches[word] = []\n",
        "\n",
        "        # for sampling batches of the first epoch\n",
        "        self.batch_counter = 0\n",
        "        self.epoch1 = True\n",
        "        \n",
        "\n",
        "    # called at end of each batch during training\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        \"\"\"Counts up a timer during the first epoch and calls tracking_neighbors after each 5000 batches\n",
        "\n",
        "        Args:\n",
        "            self: this instance of the class\n",
        "            batch(bool): indicates if we are at the end of a batch\n",
        "            logs(None)\n",
        "\n",
        "        \"\"\"\n",
        "        if self.epoch1:\n",
        "            if self.batch_counter%7000 == 0:\n",
        "                self.tracking_neighbors(self.model, batch=True)\n",
        "            self.batch_counter += 1\n",
        "        \n",
        "\n",
        "    # called at end of each epoch\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \"\"\"After each epoch calls tracking_neighbors\n",
        "\n",
        "        Args:\n",
        "            self: this instance of the class\n",
        "            epoch(int): current epoch\n",
        "            logs(None)\n",
        "\n",
        "        \"\"\"\n",
        "        self.tracking_neighbors(self.model, batch=False)\n",
        "        self.epoch1 = False # so that only in first epoch intermediate batches are sampled\n",
        "\n",
        "\n",
        "    def cosine_similarities(self, token, embeddings, embedding_units):\n",
        "        \"\"\"Calculates the cosine similarity of a word embedding to the other embeddings and returns the k nearest tokens\n",
        "\n",
        "        Args:\n",
        "            self: this instance of the class\n",
        "            token(int): an integer representing one word of the vocabulary \n",
        "            embeddings(nested list): list of all embeddings \n",
        "            embedding_units(list)\n",
        "        \n",
        "        Returns:\n",
        "            k nearest tokens\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # get embedding of specific token\n",
        "        word_embedding = embeddings[token].reshape(1,embedding_units)\n",
        "\n",
        "        # compute cosine similarity\n",
        "        cosine_similarities = cosine_similarity(embeddings, word_embedding).tolist()\n",
        "\n",
        "        # get the index of the k nearest embeddings (without the tracked token itself)\n",
        "        # index = corresponding token (as integer not word)\n",
        "        k_nearest_tokens = sorted(range(len(cosine_similarities)), key = lambda sub: cosine_similarities[sub])[-(self.k+1):-1]\n",
        "      \n",
        "        return k_nearest_tokens\n",
        "\n",
        "\n",
        "    def tracking_neighbors(self, model, batch):\n",
        "        \"\"\"Calculates the nearest neighbors of each token in the list of words to track.\n",
        "\n",
        "        Args:\n",
        "            self: this instance of the class\n",
        "            model(SkipGram): SkipGram Model that is being trained\n",
        "            batch(bool): indicates if we are at the end of a batch\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # retrieve the embeddings\n",
        "        embeddings = self.model.embedding.get_weights()[0]\n",
        "        embedding_units = self.model.embedding_units\n",
        "\n",
        "        # for token get nearest neighbors and append to dictionary under right key\n",
        "        for i, word in enumerate(self.tracking_words):\n",
        "\n",
        "            # translate word into token\n",
        "            token = tokenizer.word_index.get(word)\n",
        "            # get 'neighbor' tokens\n",
        "            nearest_tokens = self.cosine_similarities(token, embeddings, embedding_units)\n",
        "            # translate tokens back into words and reverse the list (so that closest is at first)\n",
        "            nearest_neighbors = list(reversed(tokenizer.sequences_to_texts([nearest_tokens])[0].split()))\n",
        "            \n",
        "            # update dictionary\n",
        "            if batch:\n",
        "                self.neighbors_dict_batches[word].append(nearest_neighbors)\n",
        "            else:\n",
        "                self.neighbors_dict_epochs[word].append(nearest_neighbors)\n",
        "          \n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl-EUHIeTThg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plotting function for performance metrics\n",
        "def plothist(hist):\n",
        "    print(\"\\n============== Performance of Model ================\")\n",
        "\n",
        "    line1, = plt.plot(hist.history['loss'])\n",
        "    line2, = plt.plot(hist.history['accuracy'])\n",
        "    line3, = plt.plot(history.history['val_loss'])\n",
        "    line4, = plt.plot(history.history['val_accuracy']) \n",
        "    plt.title('Model Performance')\n",
        "    plt.ylabel('Loss/Accuracy')\n",
        "    plt.xlabel('Training steps')\n",
        "    plt.legend((line1, line2, line3, line4),(\"train loss\", \"train accuracy\", \"val loss\", \"val accuracy\"))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BvLOOWlSXQb"
      },
      "outputs": [],
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "# plotting function for neighbors of tracking words\n",
        "def print_neighbors(tracking, tracking_words, epochs):\n",
        "   \n",
        "    batches_dict = tracking.neighbors_dict_batches\n",
        "    epoches_dict = tracking.neighbors_dict_epochs\n",
        "\n",
        "    print(\"\\n=============== Nearest neighbors from samples during first epoch ===============\")\n",
        "\n",
        "    # print a table for each sampled batch\n",
        "    for sample in range(len(batches_dict.get(tracking_words[0]))):\n",
        "        print(f\"Sample {sample}\")\n",
        "        \n",
        "        # iterate through dictionary\n",
        "        # print table with neighbor words of current epoch\n",
        "        table = PrettyTable()\n",
        "        columns = tracking_words\n",
        "        for i, word in enumerate(tracking_words):\n",
        "            table.add_column(columns[i], batches_dict.get(word)[sample])\n",
        "\n",
        "        print(table)   \n",
        "\n",
        "    print(\"\\n=============== Nearest neighbors at end of each epoch ===============\")\n",
        "\n",
        "    # print a table for each sampled batch\n",
        "    for epoch in range(1, epochs+1):\n",
        "        print(f\"Epoch {epoch}\")\n",
        "        \n",
        "        # iterate through dictionary\n",
        "        # print table with neighbor words of current epoch\n",
        "        table = PrettyTable()\n",
        "        columns = tracking_words\n",
        "        for i, word in enumerate(tracking_words):\n",
        "            table.add_column(columns[i], batches_dict.get(word)[epoch])\n",
        "\n",
        "        print(table)   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bWGrs_hatB9-",
        "outputId": "4ed007df-4721-4af6-9644-fa6b1d7476aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"Adam/gradients/PartitionedCall:1\", shape=(None,), dtype=int64), values=Tensor(\"Adam/gradients/PartitionedCall:0\", shape=(None, 100), dtype=float32), dense_shape=Tensor(\"Adam/gradients/PartitionedCall:2\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    5/32552 [..............................] - ETA: 7:34 - loss: 7.3432 - accuracy: 0.4812    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0119s vs `on_train_batch_end` time: 0.0264s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32552/32552 [==============================] - 294s 9ms/step - loss: 0.4995 - accuracy: 0.8482 - val_loss: 0.4431 - val_accuracy: 0.8562\n",
            "Epoch 2/2\n",
            "32552/32552 [==============================] - 279s 9ms/step - loss: 0.4246 - accuracy: 0.8609 - val_loss: 0.4102 - val_accuracy: 0.8643\n",
            "\n",
            "============== Performance of Model ================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=============== Nearest neighbors from samples during first epoch ===============\n",
            "Sample 0\n",
            "+-----------+-----------+----------+------------+-------------+----------+--------------+-----------+\n",
            "|    god    |    good   |  stone   |    hand    |     sin     |  father  |    mother    |   woman   |\n",
            "+-----------+-----------+----------+------------+-------------+----------+--------------+-----------+\n",
            "|  terraces |  blasting | watering | ramathlehi |    punon    |  beside  | unprofitable |    amal   |\n",
            "|  helping  |  lionlike |  faults  |  zebadiah  |   uttered   |   foot   |   moabite    |   weaker  |\n",
            "|  thahash  |    pole   |   out    |  repaired  |    groan    |  rigour  |   situate    |  borrowed |\n",
            "|   desert  |  gershon  |  north   |   equal    |    cockle   | fighteth |     shot     | tirshatha |\n",
            "| adversity | pasdammim | giddalti |  japhlet   | meronothite |  bewail  |   oppress    |  stilled  |\n",
            "+-----------+-----------+----------+------------+-------------+----------+--------------+-----------+\n",
            "Sample 1\n",
            "+---------+-------+-----------+----------+-------------+----------+----------+--------+\n",
            "|   god   |  good |   stone   |   hand   |     sin     |  father  |  mother  | woman  |\n",
            "+---------+-------+-----------+----------+-------------+----------+----------+--------+\n",
            "|    44   |  sons |  created  |  return  |   gathered  | brethren | captain  | cakes  |\n",
            "|  piece  |  law  | judgments |   eyes   |     tree    |    18    |  bowed   | timber |\n",
            "|  jacob  | cried |    blow   |  spoken  |  delivered  |    7     | captains | offer  |\n",
            "| vessels |  face |  removed  | children |   captains  |    23    |   esau   |  hide  |\n",
            "|    2    |  saul |  promised | sixteen  | commandment |   art    |   sin    |   52   |\n",
            "+---------+-------+-----------+----------+-------------+----------+----------+--------+\n",
            "Sample 2\n",
            "+----------+-------+-----------+----------+----------+--------+----------+----------+\n",
            "|   god    |  good |   stone   |   hand   |   sin    | father |  mother  |  woman   |\n",
            "+----------+-------+-----------+----------+----------+--------+----------+----------+\n",
            "|  joseph  |  seed | delivered |  round   | departed | jacob  | stranger | departed |\n",
            "|  house   |   27  |  unclean  | ephraim  |  samuel  |   24   |  height  |   sin    |\n",
            "|  ruler   | chief |     4     | chariots |   ram    |  pray  |    33    |    44    |\n",
            "| gathered |  lord |    wife   |  heaven  |  remain  |  thee  |  gibeah  |   ahab   |\n",
            "|    16    |  told |     36    |  dream   |  angel   |   35   |  flour   |  voice   |\n",
            "+----------+-------+-----------+----------+----------+--------+----------+----------+\n",
            "Sample 3\n",
            "+--------+------------+-------+----------+--------+----------+--------+---------+\n",
            "|  god   |    good    | stone |   hand   |  sin   |  father  | mother |  woman  |\n",
            "+--------+------------+-------+----------+--------+----------+--------+---------+\n",
            "|   23   | sacrifices |   10  |   sea    | months |  cities  |  hate  |  smote  |\n",
            "|   3    |  possess   |   25  | minister | esther |  chief   |   44   |  month  |\n",
            "| sister | judgments  |   9   |  tables  | cheek  |  offer   | family | shekels |\n",
            "| heard  |     31     |   1   | syrians  |  wing  |    14    |  sold  |  dwell  |\n",
            "|  lord  |  perfect   |  lift |  cried   |  joy   | servants | sodom  |   long  |\n",
            "+--------+------------+-------+----------+--------+----------+--------+---------+\n",
            "Sample 4\n",
            "+-------+-------+---------+--------------+------------+----------+----------+--------+\n",
            "|  god  |  good |  stone  |     hand     |    sin     |  father  |  mother  | woman  |\n",
            "+-------+-------+---------+--------------+------------+----------+----------+--------+\n",
            "|   1   | altar |   sing  |     lord     |   cities   |  blood   |  fifty   |  camp  |\n",
            "| egypt | flesh | thither |    burnt     |   mouth    |    24    | brethren | thirty |\n",
            "| shalt |   12  |   buy   |   nations    |     1      |    32    |  manner  | egypt  |\n",
            "| place |   27  | rejoice | congregation |   ruler    | joseph's |   ears   | goeth  |\n",
            "| woman |   4   |   moab  |      29      | messengers |  times   | presence | manner |\n",
            "+-------+-------+---------+--------------+------------+----------+----------+--------+\n",
            "\n",
            "=============== Nearest neighbors at end of each epoch ===============\n",
            "Epoch 1\n",
            "+---------+-------+-----------+----------+-------------+----------+----------+--------+\n",
            "|   god   |  good |   stone   |   hand   |     sin     |  father  |  mother  | woman  |\n",
            "+---------+-------+-----------+----------+-------------+----------+----------+--------+\n",
            "|    44   |  sons |  created  |  return  |   gathered  | brethren | captain  | cakes  |\n",
            "|  piece  |  law  | judgments |   eyes   |     tree    |    18    |  bowed   | timber |\n",
            "|  jacob  | cried |    blow   |  spoken  |  delivered  |    7     | captains | offer  |\n",
            "| vessels |  face |  removed  | children |   captains  |    23    |   esau   |  hide  |\n",
            "|    2    |  saul |  promised | sixteen  | commandment |   art    |   sin    |   52   |\n",
            "+---------+-------+-----------+----------+-------------+----------+----------+--------+\n",
            "Epoch 2\n",
            "+----------+-------+-----------+----------+----------+--------+----------+----------+\n",
            "|   god    |  good |   stone   |   hand   |   sin    | father |  mother  |  woman   |\n",
            "+----------+-------+-----------+----------+----------+--------+----------+----------+\n",
            "|  joseph  |  seed | delivered |  round   | departed | jacob  | stranger | departed |\n",
            "|  house   |   27  |  unclean  | ephraim  |  samuel  |   24   |  height  |   sin    |\n",
            "|  ruler   | chief |     4     | chariots |   ram    |  pray  |    33    |    44    |\n",
            "| gathered |  lord |    wife   |  heaven  |  remain  |  thee  |  gibeah  |   ahab   |\n",
            "|    16    |  told |     36    |  dream   |  angel   |   35   |  flour   |  voice   |\n",
            "+----------+-------+-----------+----------+----------+--------+----------+----------+\n"
          ]
        }
      ],
      "source": [
        "# model\n",
        "model = SkipGram(embedding_units=100)\n",
        "model.compile(optimizer='adam', \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "# words that ought to be tracked during training\n",
        "tracking_words = [\"god\", \"good\", \"stone\", \"hand\", \"sin\", \"father\", \"mother\", \"woman\"]\n",
        "# callback class with tracking function (called after each epoch)\n",
        "tracking = TrackingNeighbors(model, tracking_words, tokenizer, k=5)\n",
        "\n",
        "# index for splitting the data in training (80%) and validation set (20%)\n",
        "split = int(len(pairs) * 4/5) \n",
        "\n",
        "# training\n",
        "epochs = 5\n",
        "history = model.fit(x=pairs[:split], \n",
        "                    y=labels[:split], \n",
        "                    batch_size=64, \n",
        "                    epochs=epochs,\n",
        "                    validation_data=(pairs[split:], labels[split:]),\n",
        "                    callbacks=[tracking]\n",
        "                    )\n",
        "\n",
        "# visualization\n",
        "plothist(history)\n",
        "print_neighbors(tracking, tracking_words, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "99hOysY0pbt3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}